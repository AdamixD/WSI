{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sprawozdanie z laboratorium 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Autor: Adam Dąbkowski***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Celem szóstego laboratorium jest zaimplementowanie algorytmu ***Q-learning***. Dodatkowo należy stworzyć agenta rozwiązującego problem ***Taxi***.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Importowanie niezbędnych bibliotek"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Wizualizacja stanu środowiska"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wykorzystywane przez na środowisko zawiera cztery wyznaczone miejsca (***R***, ***G***, ***Y***, ***B***), w których pasażer może wsiąść do taksówki (***żółty prostokąt***) lub wysiąść. Gracz otrzymuje pozytywne nagrody za udane podrzucenie pasażera w odpowiednim miejscu, natomiast negatywne nagrody za próby odebrania/odwiezienia pasażera kończące się niepowodzeniem oraz za każdy krok, w którym nie otrzymano kolejnej nagrody."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m:\u001B[43m \u001B[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementacja algorytmu ***Q-learning***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Głównym zadaniem szóstego laboratorium jest implementacja algorytmu ***Q-learning***. Po za tym należy stworzyć agenta rozwiązującego problem ***Taxi***. W tym celu stworzona została klasa ***QlearningAgent***. Podczas tworzenia obiektu tej klasy istnieje możliwość podania parametrów ***env*** (*wykorzystywane środowisko*), ***beta*** (*współczynnik uczenia*), ***gamma*** (*stopa dyskontowa*) oraz ***epsilon*** (*prawdopodobieństwo $\\epsilon$*).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Klasa ***QlearningAgent*** zawiera także cztery metody:\n",
    "- ***get_parameters()*** - metoda zwracająca wartości parametrów ***beta***, ***gamma*** oraz ***epsilon***\n",
    "- ***exploration()*** - metoda odpowiadająca za strategię eksploracji (w tym przypadku ***strategię $\\epsilon$-zachłanną***)\n",
    "- ***learn()*** - metoda odpowiadająca za uczenie według algorytmu ***Q-learning***\n",
    "- ***evaluate()*** - metoda odpowiedzialna za ocenę na danym etapie uczenia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class QlearningAgent:\n",
    "    def __init__(self, env, beta=0.03, gamma=0.9, epsilon=0.01):\n",
    "        self.env = env\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return {\n",
    "            \"beta\": self.beta,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "\n",
    "    def exploration(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, n_episodes=20000, n_eval_episodes=500, eval_period=2000, deep_printing=False):\n",
    "        for i in range(n_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.exploration(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                self.Q[state, action] += self.beta * (reward + self.gamma * np.max(self.Q[new_state, :]) - self.Q[state, action])\n",
    "                state = new_state\n",
    "\n",
    "\n",
    "            if (i+1) % eval_period == 0 or (i+1) == n_episodes:\n",
    "                average_reward = self.evaluate(n_eval_episodes, deep_printing)\n",
    "                print(f'After {i+1}/{n_episodes} learning episodes - average reward: {average_reward}')\n",
    "                if deep_printing:\n",
    "                    print(\" \")\n",
    "\n",
    "        return average_reward\n",
    "\n",
    "\n",
    "    def evaluate(self, n_eval_episodes, printing=False):\n",
    "        all_rewards = []\n",
    "        for i in range(n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q[state])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            all_rewards.append(episode_reward)\n",
    "\n",
    "            if printing:\n",
    "                print(f'Episode {i} reward: {episode_reward}')\n",
    "\n",
    "        return np.mean(all_rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aby móc w łatwy sposób prezentować i analizować rezultaty działania algorytmu dla poszczególnych przypadków, zaimplementowana została prosta klasa ***Results***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.results = pd.DataFrame(columns=[\"Learning episodes\", \"beta\", \"gamma\", \"epsilon\", \"Average reward\"])\n",
    "\n",
    "    def update_results(self, n_episodes, beta, gamma, epsilon, average_reward):\n",
    "        self.results.loc[len(self.results)] = [n_episodes, beta, gamma, epsilon, average_reward]\n",
    "\n",
    "    def delete_row(self, index):\n",
    "        self.results.drop([index], axis=0, inplace=True)\n",
    "\n",
    "    def sort_results(self, column_name):\n",
    "        self.results = self.results.sort_values(by=[column_name])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.results.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Zastosowanie algorytmu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Mając zaimplementowaną klasę ***QlearningAgent*** oraz funkcje analizujące otrzymywane wyniki, możemy przejść do przeprowadzenia szeregu doświadczeń, dzięki którym zbadamy wpływ poszczególnych parametrów. Na początku ustalmy liczbę epizodów całego procesu uczenia (***n_episodes***), liczbę epizodów pojedynczej ewaluacji (***n_eval_episodes***) oraz okres, który wskazuje na częstotliwość wykonywania ewaluacji (***eval_period***). Są to wartości równe odpowiednio **20000**, **500** oraz **2000**. Poprzez zastosowanie tak dużej liczby epizodów ewaluacyjnych jesteśmy w stanie przedstawić bardziej uśrednione wyniki, a co za tym idzie, precyzyjniejsze wnioski."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "n_episodes = 20000\n",
    "n_eval_episodes = 500\n",
    "eval_period = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Badanie wpływu współczynnika $\\beta$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nasze rozważania rozpoczynamy od sprawdzenia działania algorytmu dla wartości domyślnym. W naszym przypadku są ***beta = 0,03***, ***gamma = 0,9*** oraz ***epsilon = 0,01***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "results_beta = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "{'beta': 0.03, 'gamma': 0.9, 'epsilon': 0.01}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -302.512\n",
      "After 4000/20000 learning episodes - average reward: -123.432\n",
      "After 6000/20000 learning episodes - average reward: -45.092\n",
      "After 8000/20000 learning episodes - average reward: -30.12\n",
      "After 10000/20000 learning episodes - average reward: -11.482\n",
      "After 12000/20000 learning episodes - average reward: 7.596\n",
      "After 14000/20000 learning episodes - average reward: 6.934\n",
      "After 16000/20000 learning episodes - average reward: 7.764\n",
      "After 18000/20000 learning episodes - average reward: 8.066\n",
      "After 20000/20000 learning episodes - average reward: 7.968\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jak widać, powyżej zadowalające rezultaty otrzymujemy po ok. **12000** epizodach treningowych. Sprawdźmy teraz, co się stanie, gdy wartość współczynnika uczenia $\\beta$ zostanie zwiększona do wartości **0,05**, czy algorytm będzie w stanie wcześniej osiągnąć satysfakcjonujące nas wyniki."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "beta = 0.05\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -147.908\n",
      "After 4000/20000 learning episodes - average reward: -40.524\n",
      "After 6000/20000 learning episodes - average reward: 1.622\n",
      "After 8000/20000 learning episodes - average reward: 7.556\n",
      "After 10000/20000 learning episodes - average reward: 7.994\n",
      "After 12000/20000 learning episodes - average reward: 7.898\n",
      "After 14000/20000 learning episodes - average reward: 7.842\n",
      "After 16000/20000 learning episodes - average reward: 8.168\n",
      "After 18000/20000 learning episodes - average reward: 7.842\n",
      "After 20000/20000 learning episodes - average reward: 8.074\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zgodnie z naszymi przewidywaniami, algorytm znacznie szybciej przekroczył próg o wartości **7**, ponieważ wynik ten został osiągnięty już po ok. **8000**. Chcąc otrzymać podobne rezultaty w jeszcze krótszym czasie, sprawdźmy, co się stanie, gdy współczynnik uczenia zostanie zwiększony do wartości **0,1**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -55.356\n",
      "After 4000/20000 learning episodes - average reward: 7.654\n",
      "After 6000/20000 learning episodes - average reward: 7.978\n",
      "After 8000/20000 learning episodes - average reward: 7.924\n",
      "After 10000/20000 learning episodes - average reward: 7.794\n",
      "After 12000/20000 learning episodes - average reward: 7.858\n",
      "After 14000/20000 learning episodes - average reward: 8.0\n",
      "After 16000/20000 learning episodes - average reward: 7.956\n",
      "After 18000/20000 learning episodes - average reward: 7.95\n",
      "After 20000/20000 learning episodes - average reward: 7.678\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ponownie czas, po którym otrzymujemy świetne wyniki, został skrócony, tym razem do ok. **4000** epizodów. Oczywiście możemy domyślać się, że próg równy **7** został uzyskany znacznie wcześniej, gdyż przyjęty kwant czasu (w naszym przypadku liczbę epizodów uczących pomiędzy ewaluacjami) uniemożliwia dostrzeżenie wspomnianej obserwacji. Aby to udowodnić, poniżej zamieszczono wyniki doświadczenia dla mniejszej wartości ***eval_period*** równej **500**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 500/6000 learning episodes - average reward: -228.088\n",
      "After 1000/6000 learning episodes - average reward: -214.396\n",
      "After 1500/6000 learning episodes - average reward: -71.064\n",
      "After 2000/6000 learning episodes - average reward: -17.962\n",
      "After 2500/6000 learning episodes - average reward: -33.15\n",
      "After 3000/6000 learning episodes - average reward: -2.002\n",
      "After 3500/6000 learning episodes - average reward: 7.22\n",
      "After 4000/6000 learning episodes - average reward: 6.164\n",
      "After 4500/6000 learning episodes - average reward: 8.228\n",
      "After 5000/6000 learning episodes - average reward: 8.004\n",
      "After 5500/6000 learning episodes - average reward: 7.966\n",
      "After 6000/6000 learning episodes - average reward: 8.064\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=6000, n_eval_episodes=500, eval_period=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na koniec sprawdźmy, czy zmniejszenie współczynnika uczenia rzeczywiście poskutkuje znacznie wolniejszym przebiegiem uczenia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -343.19\n",
      "After 4000/20000 learning episodes - average reward: -382.952\n",
      "After 6000/20000 learning episodes - average reward: -292.628\n",
      "After 8000/20000 learning episodes - average reward: -267.482\n",
      "After 10000/20000 learning episodes - average reward: -418.016\n",
      "After 12000/20000 learning episodes - average reward: -278.876\n",
      "After 14000/20000 learning episodes - average reward: -318.242\n",
      "After 16000/20000 learning episodes - average reward: -264.62\n",
      "After 18000/20000 learning episodes - average reward: -307.352\n",
      "After 20000/20000 learning episodes - average reward: -264.242\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jak widać, nasze przypuszczenia ponownie się potwierdziły. Dla ***beta*** równej **0,001** algorytm nie jest w stanie nawet osiągnąć wartości dodatniej."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poniżej zamieszona jest zbiorcza tabela zawierająca wyniki dotychczasowych doświadczeń."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "results_beta.sort_results(\"beta\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward\n3            20000.0  0.001    0.9     0.01        -264.242\n0            20000.0  0.030    0.9     0.01           7.968\n1            20000.0  0.050    0.9     0.01           8.074\n2            20000.0  0.100    0.9     0.01           7.678",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>-264.242</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>8.074</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_beta.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Badanie wpływu współczynnika $\\gamma$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym z badanych parametrów jest stopa dyskontowa **$\\gamma$**. Jeśli jej wartość będzie bliska **0**, wówczas przyszłe nagrody będą miały marginalne znaczenie w stosunku do bieżących nagród, natomiast jeśli **$\\gamma$** będzie przyjmować wartości zbliżające się do **1**, to przyszłe nagrody będą miały niemal identyczne znaczenie co nagrody bieżące."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na początku sprawdźmy, czy zwiększenie stopy dyskontowej będzie pozytywnie wpływało na szybkość uczenia. W tym celu przeprowadzimy trzy doświadczenia, w których $\\gamma$ równa będzie **0,95**, **0,99** oraz **0,999**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "results_gamma = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.95\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -306.65\n",
      "After 4000/20000 learning episodes - average reward: -131.518\n",
      "After 6000/20000 learning episodes - average reward: -63.67\n",
      "After 8000/20000 learning episodes - average reward: 2.188\n",
      "After 10000/20000 learning episodes - average reward: 6.752\n",
      "After 12000/20000 learning episodes - average reward: 7.808\n",
      "After 14000/20000 learning episodes - average reward: 8.07\n",
      "After 16000/20000 learning episodes - average reward: 7.812\n",
      "After 18000/20000 learning episodes - average reward: 8.0\n",
      "After 20000/20000 learning episodes - average reward: 8.006\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.99\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.096\n",
      "After 4000/20000 learning episodes - average reward: -110.636\n",
      "After 6000/20000 learning episodes - average reward: -25.574\n",
      "After 8000/20000 learning episodes - average reward: 7.82\n",
      "After 10000/20000 learning episodes - average reward: 7.82\n",
      "After 12000/20000 learning episodes - average reward: 8.086\n",
      "After 14000/20000 learning episodes - average reward: 7.982\n",
      "After 16000/20000 learning episodes - average reward: 7.988\n",
      "After 18000/20000 learning episodes - average reward: 7.854\n",
      "After 20000/20000 learning episodes - average reward: 7.824\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.999\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.45\n",
      "After 4000/20000 learning episodes - average reward: -92.066\n",
      "After 6000/20000 learning episodes - average reward: -58.87\n",
      "After 8000/20000 learning episodes - average reward: 4.662\n",
      "After 10000/20000 learning episodes - average reward: 7.952\n",
      "After 12000/20000 learning episodes - average reward: 7.856\n",
      "After 14000/20000 learning episodes - average reward: 7.926\n",
      "After 16000/20000 learning episodes - average reward: 8.018\n",
      "After 18000/20000 learning episodes - average reward: 8.026\n",
      "After 20000/20000 learning episodes - average reward: 7.934\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O ile nie dostrzegamy poważnych różnic pomiędzy wynikami dla trzech powyższych symulacji, o tyle łatwo zauważyć istotną poprawę względem wartości, które przyjęliśmy za domyślne. Sprawdźmy zatem, czy zmniejszenie współczynnika ***gamma*** względem wartości początkowej, będzie negatywne w skutkach."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.8\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -203.434\n",
      "After 4000/20000 learning episodes - average reward: -153.17\n",
      "After 6000/20000 learning episodes - average reward: -90.02\n",
      "After 8000/20000 learning episodes - average reward: -47.838\n",
      "After 10000/20000 learning episodes - average reward: -21.296\n",
      "After 12000/20000 learning episodes - average reward: -12.718\n",
      "After 14000/20000 learning episodes - average reward: -9.374\n",
      "After 16000/20000 learning episodes - average reward: 1.662\n",
      "After 18000/20000 learning episodes - average reward: 2.184\n",
      "After 20000/20000 learning episodes - average reward: 1.814\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.6\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -214.526\n",
      "After 4000/20000 learning episodes - average reward: -154.838\n",
      "After 6000/20000 learning episodes - average reward: -131.562\n",
      "After 8000/20000 learning episodes - average reward: -117.354\n",
      "After 10000/20000 learning episodes - average reward: -86.134\n",
      "After 12000/20000 learning episodes - average reward: -67.1\n",
      "After 14000/20000 learning episodes - average reward: -75.028\n",
      "After 16000/20000 learning episodes - average reward: -47.654\n",
      "After 18000/20000 learning episodes - average reward: -44.388\n",
      "After 20000/20000 learning episodes - average reward: -38.094\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zarówno w jednym, jak i drugim przypadku wyniki są zauważalnie gorsze, jednakże dla stopy równej **0,6** w przeciwieństwie do sytuacji, gdy jest ona równa **0,8**, nie jesteśmy w stanie uzyskać wyniku dodatniego w danej liczbie epizodów procesu uczenia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "results_gamma.sort_results(\"gamma\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward\n4            20000.0  0.03  0.600     0.01         -38.094\n3            20000.0  0.03  0.800     0.01           1.814\n0            20000.0  0.03  0.950     0.01           8.006\n1            20000.0  0.03  0.990     0.01           7.824\n2            20000.0  0.03  0.999     0.01           7.934",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.600</td>\n      <td>0.01</td>\n      <td>-38.094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.800</td>\n      <td>0.01</td>\n      <td>1.814</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.950</td>\n      <td>0.01</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.990</td>\n      <td>0.01</td>\n      <td>7.824</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.999</td>\n      <td>0.01</td>\n      <td>7.934</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gamma.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Badanie wpływu wartości parametru $\\epsilon$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mając na uwadze fakt, że przyjętą przez nas strategią eksploracji jest strategia $\\epsilon$ - zachłanna, istotnym parametrem jest $\\epsilon$ - prawdopodobieństwo wyboru losowego, przy prawdopodobieństwie wyboru czynności o największej Q-wartości równym 1-$\\epsilon$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chcąc sprawdzić, czy zwiększenie eksploracji poprzez zwiększenie losowości pozytywnie wpłynie na czas otrzymywania satysfakcjonujących wyników, przeprowadzone zostaną doświadczenia o parametrze ***epsilon*** równym kolejno **0,05**, **0,1**, **0,2**, **0,5** oraz **1**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "results_epsilon = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -167.262\n",
      "After 4000/20000 learning episodes - average reward: -104.718\n",
      "After 6000/20000 learning episodes - average reward: -31.56\n",
      "After 8000/20000 learning episodes - average reward: -20.166\n",
      "After 10000/20000 learning episodes - average reward: 4.742\n",
      "After 12000/20000 learning episodes - average reward: 7.618\n",
      "After 14000/20000 learning episodes - average reward: 7.984\n",
      "After 16000/20000 learning episodes - average reward: 7.87\n",
      "After 18000/20000 learning episodes - average reward: 7.922\n",
      "After 20000/20000 learning episodes - average reward: 8.006\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -192.81\n",
      "After 4000/20000 learning episodes - average reward: -88.974\n",
      "After 6000/20000 learning episodes - average reward: -33.29\n",
      "After 8000/20000 learning episodes - average reward: -8.498\n",
      "After 10000/20000 learning episodes - average reward: 1.834\n",
      "After 12000/20000 learning episodes - average reward: 4.466\n",
      "After 14000/20000 learning episodes - average reward: 7.686\n",
      "After 16000/20000 learning episodes - average reward: 7.712\n",
      "After 18000/20000 learning episodes - average reward: 7.78\n",
      "After 20000/20000 learning episodes - average reward: 7.868\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -172.422\n",
      "After 4000/20000 learning episodes - average reward: -86.99\n",
      "After 6000/20000 learning episodes - average reward: -25.952\n",
      "After 8000/20000 learning episodes - average reward: -11.02\n",
      "After 10000/20000 learning episodes - average reward: 2.748\n",
      "After 12000/20000 learning episodes - average reward: 8.03\n",
      "After 14000/20000 learning episodes - average reward: 7.984\n",
      "After 16000/20000 learning episodes - average reward: 7.856\n",
      "After 18000/20000 learning episodes - average reward: 7.922\n",
      "After 20000/20000 learning episodes - average reward: 7.772\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -168.276\n",
      "After 4000/20000 learning episodes - average reward: -71.186\n",
      "After 6000/20000 learning episodes - average reward: -6.908\n",
      "After 8000/20000 learning episodes - average reward: -0.346\n",
      "After 10000/20000 learning episodes - average reward: 5.224\n",
      "After 12000/20000 learning episodes - average reward: 7.834\n",
      "After 14000/20000 learning episodes - average reward: 7.722\n",
      "After 16000/20000 learning episodes - average reward: 7.936\n",
      "After 18000/20000 learning episodes - average reward: 7.86\n",
      "After 20000/20000 learning episodes - average reward: 7.902\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -175.062\n",
      "After 4000/20000 learning episodes - average reward: 7.608\n",
      "After 6000/20000 learning episodes - average reward: 7.92\n",
      "After 8000/20000 learning episodes - average reward: 7.976\n",
      "After 10000/20000 learning episodes - average reward: 7.882\n",
      "After 12000/20000 learning episodes - average reward: 7.93\n",
      "After 14000/20000 learning episodes - average reward: 7.868\n",
      "After 16000/20000 learning episodes - average reward: 7.802\n",
      "After 18000/20000 learning episodes - average reward: 8.026\n",
      "After 20000/20000 learning episodes - average reward: 7.906\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otrzymane wyniki w większości nie odbiegają znacząco od tych, które otrzymaliśmy dla wartości początkowych, ponieważ zadowalające wartości otrzymujemy w przybliżeniu po ok. **12000** epizodach uczenia. Ciekawe rezultaty otrzymaliśmy natomiast dla ***epsilona*** równego 1, wskazującego na całkowicie losowe podejście do strategi eksploracji. O dziwo wyniki są rewelacyjne. Jednakże wynika to głównie ze szczęśliwego trafu i specyfiki problemu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wiedząc, jaki wpływ ma duża wartość parametru ***epsilon***, sprawdźmy, co się stanie, gdy losowość podczas eksploracji zostanie zmniejszona. W tym celu przeprowadzone zostaną doświadczenia dla $\\epsilon$ równego **0,005**, **0,001**, **0,0001** oraz **0**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.005"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -361.53\n",
      "After 4000/20000 learning episodes - average reward: -123.074\n",
      "After 6000/20000 learning episodes - average reward: -43.53\n",
      "After 8000/20000 learning episodes - average reward: -4.774\n",
      "After 10000/20000 learning episodes - average reward: 2.23\n",
      "After 12000/20000 learning episodes - average reward: 5.758\n",
      "After 14000/20000 learning episodes - average reward: 7.96\n",
      "After 16000/20000 learning episodes - average reward: 7.888\n",
      "After 18000/20000 learning episodes - average reward: 7.93\n",
      "After 20000/20000 learning episodes - average reward: 8.062\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -215.634\n",
      "After 4000/20000 learning episodes - average reward: -113.16\n",
      "After 6000/20000 learning episodes - average reward: -28.83\n",
      "After 8000/20000 learning episodes - average reward: 0.902\n",
      "After 10000/20000 learning episodes - average reward: 2.248\n",
      "After 12000/20000 learning episodes - average reward: 2.378\n",
      "After 14000/20000 learning episodes - average reward: 7.69\n",
      "After 16000/20000 learning episodes - average reward: 7.952\n",
      "After 18000/20000 learning episodes - average reward: 7.98\n",
      "After 20000/20000 learning episodes - average reward: 8.022\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.76\n",
      "After 4000/20000 learning episodes - average reward: -126.51\n",
      "After 6000/20000 learning episodes - average reward: -34.206\n",
      "After 8000/20000 learning episodes - average reward: -3.952\n",
      "After 10000/20000 learning episodes - average reward: -15.384\n",
      "After 12000/20000 learning episodes - average reward: 8.038\n",
      "After 14000/20000 learning episodes - average reward: 6.818\n",
      "After 16000/20000 learning episodes - average reward: 8.16\n",
      "After 18000/20000 learning episodes - average reward: 7.714\n",
      "After 20000/20000 learning episodes - average reward: 7.988\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -174.01\n",
      "After 4000/20000 learning episodes - average reward: -68.62\n",
      "After 6000/20000 learning episodes - average reward: -23.978\n",
      "After 8000/20000 learning episodes - average reward: -0.548\n",
      "After 10000/20000 learning episodes - average reward: 2.138\n",
      "After 12000/20000 learning episodes - average reward: 7.476\n",
      "After 14000/20000 learning episodes - average reward: 6.52\n",
      "After 16000/20000 learning episodes - average reward: 8.012\n",
      "After 18000/20000 learning episodes - average reward: 8.1\n",
      "After 20000/20000 learning episodes - average reward: 7.748\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ponownie jak wcześniej, otrzymane rezultaty przypominają te, które obserwowaliśmy dla wartości początkowych. Dzięki temu wiemy, żeby wpływ parametru $\\epsilon$ jest tak duży jak ma to miejsce dla współczynnika uczenia czy stopy dyskontowej."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poniżej, analogicznie jak wcześniej, zamieszczona została tabela zawierająca wyniki przeprowadzone w omawianym podpunkcie."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "results_epsilon.sort_results(\"epsilon\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward\n8            20000.0  0.03    0.9   0.0000           7.748\n7            20000.0  0.03    0.9   0.0001           7.988\n6            20000.0  0.03    0.9   0.0010           8.022\n5            20000.0  0.03    0.9   0.0050           8.062\n0            20000.0  0.03    0.9   0.0500           8.006\n1            20000.0  0.03    0.9   0.1000           7.868\n2            20000.0  0.03    0.9   0.2000           7.772\n3            20000.0  0.03    0.9   0.5000           7.902\n4            20000.0  0.03    0.9   1.0000           7.906",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0000</td>\n      <td>7.748</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0001</td>\n      <td>7.988</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0010</td>\n      <td>8.022</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0050</td>\n      <td>8.062</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0500</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.1000</td>\n      <td>7.868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.2000</td>\n      <td>7.772</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.5000</td>\n      <td>7.902</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>1.0000</td>\n      <td>7.906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_epsilon.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Podsumowanie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Dla zwiększenia czytelności otrzymanych rezultatów, poniżej została zamieszczona zbiorcza tabela, która przedstawia wyniki powyższych doświadczeń."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "results = pd.concat([results_beta.results, results_gamma.results, results_epsilon.results])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward\n3            20000.0  0.001  0.900   0.0100        -264.242\n0            20000.0  0.030  0.900   0.0100           7.968\n1            20000.0  0.050  0.900   0.0100           8.074\n2            20000.0  0.100  0.900   0.0100           7.678\n4            20000.0  0.030  0.600   0.0100         -38.094\n3            20000.0  0.030  0.800   0.0100           1.814\n0            20000.0  0.030  0.950   0.0100           8.006\n1            20000.0  0.030  0.990   0.0100           7.824\n2            20000.0  0.030  0.999   0.0100           7.934\n8            20000.0  0.030  0.900   0.0000           7.748\n7            20000.0  0.030  0.900   0.0001           7.988\n6            20000.0  0.030  0.900   0.0010           8.022\n5            20000.0  0.030  0.900   0.0050           8.062\n0            20000.0  0.030  0.900   0.0500           8.006\n1            20000.0  0.030  0.900   0.1000           7.868\n2            20000.0  0.030  0.900   0.2000           7.772\n3            20000.0  0.030  0.900   0.5000           7.902\n4            20000.0  0.030  0.900   1.0000           7.906",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>-264.242</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>8.074</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.678</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.600</td>\n      <td>0.0100</td>\n      <td>-38.094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.800</td>\n      <td>0.0100</td>\n      <td>1.814</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.950</td>\n      <td>0.0100</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.990</td>\n      <td>0.0100</td>\n      <td>7.824</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.999</td>\n      <td>0.0100</td>\n      <td>7.934</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0000</td>\n      <td>7.748</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0001</td>\n      <td>7.988</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0010</td>\n      <td>8.022</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0050</td>\n      <td>8.062</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0500</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.1000</td>\n      <td>7.868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.2000</td>\n      <td>7.772</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.5000</td>\n      <td>7.902</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>1.0000</td>\n      <td>7.906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO - końcowe wnioski"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}