{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sprawozdanie z laboratorium 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Autor: Adam Dąbkowski***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Celem szóstego laboratorium jest zaimplementowanie algorytmu ***Q-learning***. Dodatkowo należy stworzyć agenta rozwiązującego problem ***Taxi***.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Importowanie niezbędnych bibliotek"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Wizualizacja stanu środowiska"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wykorzystywane przez na środowisko zawiera cztery wyznaczone miejsca (***R***, ***G***, ***Y***, ***B***), w których pasażer może wsiąść do taksówki (***żółty prostokąt***) lub wysiąść. Gracz otrzymuje pozytywne nagrody za udane podrzucenie pasażera w odpowiednim miejscu, natomiast negatywne nagrody za próby odebrania/odwiezienia pasażera kończące się niepowodzeniem oraz za każdy krok, w którym nie otrzymano kolejnej nagrody."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :\u001B[34;1mG\u001B[0m|\n",
      "| : |\u001B[43m \u001B[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementacja algorytmu ***Q-learning***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Głównym zadaniem szóstego laboratorium jest implementacja algorytmu ***Q-learning***. Po za tym należy stworzyć agenta rozwiązującego problem ***Taxi***. W tym celu stworzona została klasa ***QlearningAgent***. Podczas tworzenia obiektu tej klasy istnieje możliwość podania parametrów ***env*** (*wykorzystywane środowisko*), ***beta*** (*współczynnik uczenia*), ***gamma*** (*stopa dyskontowa*) oraz ***epsilon*** (*prawdopodobieństwo $\\epsilon$*).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Klasa ***QlearningAgent*** zawiera także cztery metody:\n",
    "- ***get_parameters()*** - metoda zwracająca wartości parametrów ***beta***, ***gamma*** oraz ***epsilon***\n",
    "- ***exploration()*** - metoda odpowiadająca za strategię eksploracji (w tym przypadku ***strategię $\\epsilon$-zachłanną***)\n",
    "- ***learn()*** - metoda odpowiadająca za uczenie według algorytmu ***Q-learning***\n",
    "- ***evaluate()*** - metoda odpowiedzialna za ocenę na danym etapie uczenia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class QlearningAgent:\n",
    "    def __init__(self, env, beta=0.03, gamma=0.9, epsilon=0.01):\n",
    "        self.env = env\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return {\n",
    "            \"beta\": self.beta,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "\n",
    "    def exploration(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, n_episodes=10000, n_eval_episodes=20, eval_period=2000, deep_printing=False, plot_history=True):\n",
    "        # all_rewards = []\n",
    "        for i in range(n_episodes):\n",
    "            # episode_reward = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.exploration(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                self.Q[state, action] += self.beta * (reward + self.gamma * np.max(self.Q[new_state, :]) - self.Q[state, action])\n",
    "                # episode_reward += reward\n",
    "                state = new_state\n",
    "\n",
    "            # all_rewards.append(episode_reward)\n",
    "\n",
    "            if (i+1) % eval_period == 0 or (i+1) == n_episodes:\n",
    "                average_reward = self.evaluate(n_eval_episodes, deep_printing)\n",
    "                print(f'After {i+1}/{n_episodes} learning episodes - average reward: {average_reward}')\n",
    "                if deep_printing:\n",
    "                    print(\" \")\n",
    "\n",
    "        # if plot_history:\n",
    "        #     plt.plot(all_rewards)\n",
    "\n",
    "        return average_reward\n",
    "\n",
    "\n",
    "    def evaluate(self, n_eval_episodes, printing=False):\n",
    "        all_rewards = []\n",
    "        for i in range(n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q[state])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            all_rewards.append(episode_reward)\n",
    "\n",
    "            if printing:\n",
    "                print(f'Episode {i} reward: {episode_reward}')\n",
    "\n",
    "        return np.mean(all_rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aby móc w łatwy sposób prezentować i analizować rezultaty działania algorytmu dla poszczególnych przypadków, zaimplementowana została prosta klasa ***Results***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.results = pd.DataFrame(columns=[\"Learning episodes\", \"beta\", \"gamma\", \"epsilon\", \"Average reward\"])\n",
    "\n",
    "    def update_results(self, n_episodes, beta, gamma, epsilon, average_reward):\n",
    "        self.results.loc[len(self.results)] = [n_episodes, beta, gamma, epsilon, average_reward]\n",
    "\n",
    "    def delete_row(self, index):\n",
    "        self.results.drop([index], axis=0, inplace=True)\n",
    "\n",
    "    def sort_results(self, column_name):\n",
    "        self.results = self.results.sort_values(by=[column_name])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.results.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Zastosowanie algorytmu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "n_episodes = 20000\n",
    "n_eval_episodes = 500\n",
    "eval_period = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Badanie wpływu współczynnika $\\beta$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "results_beta = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "{'beta': 0.03, 'gamma': 0.9, 'epsilon': 0.01}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -302.512\n",
      "After 4000/20000 learning episodes - average reward: -123.432\n",
      "After 6000/20000 learning episodes - average reward: -45.092\n",
      "After 8000/20000 learning episodes - average reward: -30.12\n",
      "After 10000/20000 learning episodes - average reward: -11.482\n",
      "After 12000/20000 learning episodes - average reward: 7.596\n",
      "After 14000/20000 learning episodes - average reward: 6.934\n",
      "After 16000/20000 learning episodes - average reward: 7.764\n",
      "After 18000/20000 learning episodes - average reward: 8.066\n",
      "After 20000/20000 learning episodes - average reward: 7.968\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "beta = 0.05\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -147.908\n",
      "After 4000/20000 learning episodes - average reward: -40.524\n",
      "After 6000/20000 learning episodes - average reward: 1.622\n",
      "After 8000/20000 learning episodes - average reward: 7.556\n",
      "After 10000/20000 learning episodes - average reward: 7.994\n",
      "After 12000/20000 learning episodes - average reward: 7.898\n",
      "After 14000/20000 learning episodes - average reward: 7.842\n",
      "After 16000/20000 learning episodes - average reward: 8.168\n",
      "After 18000/20000 learning episodes - average reward: 7.842\n",
      "After 20000/20000 learning episodes - average reward: 8.074\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -55.356\n",
      "After 4000/20000 learning episodes - average reward: 7.654\n",
      "After 6000/20000 learning episodes - average reward: 7.978\n",
      "After 8000/20000 learning episodes - average reward: 7.924\n",
      "After 10000/20000 learning episodes - average reward: 7.794\n",
      "After 12000/20000 learning episodes - average reward: 7.858\n",
      "After 14000/20000 learning episodes - average reward: 8.0\n",
      "After 16000/20000 learning episodes - average reward: 7.956\n",
      "After 18000/20000 learning episodes - average reward: 7.95\n",
      "After 20000/20000 learning episodes - average reward: 7.678\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -343.19\n",
      "After 4000/20000 learning episodes - average reward: -382.952\n",
      "After 6000/20000 learning episodes - average reward: -292.628\n",
      "After 8000/20000 learning episodes - average reward: -267.482\n",
      "After 10000/20000 learning episodes - average reward: -418.016\n",
      "After 12000/20000 learning episodes - average reward: -278.876\n",
      "After 14000/20000 learning episodes - average reward: -318.242\n",
      "After 16000/20000 learning episodes - average reward: -264.62\n",
      "After 18000/20000 learning episodes - average reward: -307.352\n",
      "After 20000/20000 learning episodes - average reward: -264.242\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "results_beta.sort_results(\"beta\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward\n3            20000.0  0.001    0.9     0.01        -264.242\n0            20000.0  0.030    0.9     0.01           7.968\n1            20000.0  0.050    0.9     0.01           8.074\n2            20000.0  0.100    0.9     0.01           7.678",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>-264.242</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>8.074</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_beta.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Badanie wpływu współczynnika $\\gamma$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "results_gamma = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.95\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -306.65\n",
      "After 4000/20000 learning episodes - average reward: -131.518\n",
      "After 6000/20000 learning episodes - average reward: -63.67\n",
      "After 8000/20000 learning episodes - average reward: 2.188\n",
      "After 10000/20000 learning episodes - average reward: 6.752\n",
      "After 12000/20000 learning episodes - average reward: 7.808\n",
      "After 14000/20000 learning episodes - average reward: 8.07\n",
      "After 16000/20000 learning episodes - average reward: 7.812\n",
      "After 18000/20000 learning episodes - average reward: 8.0\n",
      "After 20000/20000 learning episodes - average reward: 8.006\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.99\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.096\n",
      "After 4000/20000 learning episodes - average reward: -110.636\n",
      "After 6000/20000 learning episodes - average reward: -25.574\n",
      "After 8000/20000 learning episodes - average reward: 7.82\n",
      "After 10000/20000 learning episodes - average reward: 7.82\n",
      "After 12000/20000 learning episodes - average reward: 8.086\n",
      "After 14000/20000 learning episodes - average reward: 7.982\n",
      "After 16000/20000 learning episodes - average reward: 7.988\n",
      "After 18000/20000 learning episodes - average reward: 7.854\n",
      "After 20000/20000 learning episodes - average reward: 7.824\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.999\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.45\n",
      "After 4000/20000 learning episodes - average reward: -92.066\n",
      "After 6000/20000 learning episodes - average reward: -58.87\n",
      "After 8000/20000 learning episodes - average reward: 4.662\n",
      "After 10000/20000 learning episodes - average reward: 7.952\n",
      "After 12000/20000 learning episodes - average reward: 7.856\n",
      "After 14000/20000 learning episodes - average reward: 7.926\n",
      "After 16000/20000 learning episodes - average reward: 8.018\n",
      "After 18000/20000 learning episodes - average reward: 8.026\n",
      "After 20000/20000 learning episodes - average reward: 7.934\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.8\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -203.434\n",
      "After 4000/20000 learning episodes - average reward: -153.17\n",
      "After 6000/20000 learning episodes - average reward: -90.02\n",
      "After 8000/20000 learning episodes - average reward: -47.838\n",
      "After 10000/20000 learning episodes - average reward: -21.296\n",
      "After 12000/20000 learning episodes - average reward: -12.718\n",
      "After 14000/20000 learning episodes - average reward: -9.374\n",
      "After 16000/20000 learning episodes - average reward: 1.662\n",
      "After 18000/20000 learning episodes - average reward: 2.184\n",
      "After 20000/20000 learning episodes - average reward: 1.814\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.6\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -214.526\n",
      "After 4000/20000 learning episodes - average reward: -154.838\n",
      "After 6000/20000 learning episodes - average reward: -131.562\n",
      "After 8000/20000 learning episodes - average reward: -117.354\n",
      "After 10000/20000 learning episodes - average reward: -86.134\n",
      "After 12000/20000 learning episodes - average reward: -67.1\n",
      "After 14000/20000 learning episodes - average reward: -75.028\n",
      "After 16000/20000 learning episodes - average reward: -47.654\n",
      "After 18000/20000 learning episodes - average reward: -44.388\n",
      "After 20000/20000 learning episodes - average reward: -38.094\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "results_gamma.sort_results(\"gamma\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward\n4            20000.0  0.03  0.600     0.01         -38.094\n3            20000.0  0.03  0.800     0.01           1.814\n0            20000.0  0.03  0.950     0.01           8.006\n1            20000.0  0.03  0.990     0.01           7.824\n2            20000.0  0.03  0.999     0.01           7.934",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.600</td>\n      <td>0.01</td>\n      <td>-38.094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.800</td>\n      <td>0.01</td>\n      <td>1.814</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.950</td>\n      <td>0.01</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.990</td>\n      <td>0.01</td>\n      <td>7.824</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.999</td>\n      <td>0.01</td>\n      <td>7.934</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gamma.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Badanie wpływu wartości parametru $\\epsilon$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "results_epsilon = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -167.262\n",
      "After 4000/20000 learning episodes - average reward: -104.718\n",
      "After 6000/20000 learning episodes - average reward: -31.56\n",
      "After 8000/20000 learning episodes - average reward: -20.166\n",
      "After 10000/20000 learning episodes - average reward: 4.742\n",
      "After 12000/20000 learning episodes - average reward: 7.618\n",
      "After 14000/20000 learning episodes - average reward: 7.984\n",
      "After 16000/20000 learning episodes - average reward: 7.87\n",
      "After 18000/20000 learning episodes - average reward: 7.922\n",
      "After 20000/20000 learning episodes - average reward: 8.006\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -192.81\n",
      "After 4000/20000 learning episodes - average reward: -88.974\n",
      "After 6000/20000 learning episodes - average reward: -33.29\n",
      "After 8000/20000 learning episodes - average reward: -8.498\n",
      "After 10000/20000 learning episodes - average reward: 1.834\n",
      "After 12000/20000 learning episodes - average reward: 4.466\n",
      "After 14000/20000 learning episodes - average reward: 7.686\n",
      "After 16000/20000 learning episodes - average reward: 7.712\n",
      "After 18000/20000 learning episodes - average reward: 7.78\n",
      "After 20000/20000 learning episodes - average reward: 7.868\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -172.422\n",
      "After 4000/20000 learning episodes - average reward: -86.99\n",
      "After 6000/20000 learning episodes - average reward: -25.952\n",
      "After 8000/20000 learning episodes - average reward: -11.02\n",
      "After 10000/20000 learning episodes - average reward: 2.748\n",
      "After 12000/20000 learning episodes - average reward: 8.03\n",
      "After 14000/20000 learning episodes - average reward: 7.984\n",
      "After 16000/20000 learning episodes - average reward: 7.856\n",
      "After 18000/20000 learning episodes - average reward: 7.922\n",
      "After 20000/20000 learning episodes - average reward: 7.772\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -168.276\n",
      "After 4000/20000 learning episodes - average reward: -71.186\n",
      "After 6000/20000 learning episodes - average reward: -6.908\n",
      "After 8000/20000 learning episodes - average reward: -0.346\n",
      "After 10000/20000 learning episodes - average reward: 5.224\n",
      "After 12000/20000 learning episodes - average reward: 7.834\n",
      "After 14000/20000 learning episodes - average reward: 7.722\n",
      "After 16000/20000 learning episodes - average reward: 7.936\n",
      "After 18000/20000 learning episodes - average reward: 7.86\n",
      "After 20000/20000 learning episodes - average reward: 7.902\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -175.062\n",
      "After 4000/20000 learning episodes - average reward: 7.608\n",
      "After 6000/20000 learning episodes - average reward: 7.92\n",
      "After 8000/20000 learning episodes - average reward: 7.976\n",
      "After 10000/20000 learning episodes - average reward: 7.882\n",
      "After 12000/20000 learning episodes - average reward: 7.93\n",
      "After 14000/20000 learning episodes - average reward: 7.868\n",
      "After 16000/20000 learning episodes - average reward: 7.802\n",
      "After 18000/20000 learning episodes - average reward: 8.026\n",
      "After 20000/20000 learning episodes - average reward: 7.906\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.005"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -361.53\n",
      "After 4000/20000 learning episodes - average reward: -123.074\n",
      "After 6000/20000 learning episodes - average reward: -43.53\n",
      "After 8000/20000 learning episodes - average reward: -4.774\n",
      "After 10000/20000 learning episodes - average reward: 2.23\n",
      "After 12000/20000 learning episodes - average reward: 5.758\n",
      "After 14000/20000 learning episodes - average reward: 7.96\n",
      "After 16000/20000 learning episodes - average reward: 7.888\n",
      "After 18000/20000 learning episodes - average reward: 7.93\n",
      "After 20000/20000 learning episodes - average reward: 8.062\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -215.634\n",
      "After 4000/20000 learning episodes - average reward: -113.16\n",
      "After 6000/20000 learning episodes - average reward: -28.83\n",
      "After 8000/20000 learning episodes - average reward: 0.902\n",
      "After 10000/20000 learning episodes - average reward: 2.248\n",
      "After 12000/20000 learning episodes - average reward: 2.378\n",
      "After 14000/20000 learning episodes - average reward: 7.69\n",
      "After 16000/20000 learning episodes - average reward: 7.952\n",
      "After 18000/20000 learning episodes - average reward: 7.98\n",
      "After 20000/20000 learning episodes - average reward: 8.022\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -247.76\n",
      "After 4000/20000 learning episodes - average reward: -126.51\n",
      "After 6000/20000 learning episodes - average reward: -34.206\n",
      "After 8000/20000 learning episodes - average reward: -3.952\n",
      "After 10000/20000 learning episodes - average reward: -15.384\n",
      "After 12000/20000 learning episodes - average reward: 8.038\n",
      "After 14000/20000 learning episodes - average reward: 6.818\n",
      "After 16000/20000 learning episodes - average reward: 8.16\n",
      "After 18000/20000 learning episodes - average reward: 7.714\n",
      "After 20000/20000 learning episodes - average reward: 7.988\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -174.01\n",
      "After 4000/20000 learning episodes - average reward: -68.62\n",
      "After 6000/20000 learning episodes - average reward: -23.978\n",
      "After 8000/20000 learning episodes - average reward: -0.548\n",
      "After 10000/20000 learning episodes - average reward: 2.138\n",
      "After 12000/20000 learning episodes - average reward: 7.476\n",
      "After 14000/20000 learning episodes - average reward: 6.52\n",
      "After 16000/20000 learning episodes - average reward: 8.012\n",
      "After 18000/20000 learning episodes - average reward: 8.1\n",
      "After 20000/20000 learning episodes - average reward: 7.748\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "results_epsilon.sort_results(\"epsilon\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward\n8            20000.0  0.03    0.9   0.0000           7.748\n7            20000.0  0.03    0.9   0.0001           7.988\n6            20000.0  0.03    0.9   0.0010           8.022\n5            20000.0  0.03    0.9   0.0050           8.062\n0            20000.0  0.03    0.9   0.0500           8.006\n1            20000.0  0.03    0.9   0.1000           7.868\n2            20000.0  0.03    0.9   0.2000           7.772\n3            20000.0  0.03    0.9   0.5000           7.902\n4            20000.0  0.03    0.9   1.0000           7.906",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0000</td>\n      <td>7.748</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0001</td>\n      <td>7.988</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0010</td>\n      <td>8.022</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0050</td>\n      <td>8.062</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0500</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.1000</td>\n      <td>7.868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.2000</td>\n      <td>7.772</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.5000</td>\n      <td>7.902</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>1.0000</td>\n      <td>7.906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_epsilon.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Podsumowanie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "results = pd.concat([results_beta.results, results_gamma.results, results_epsilon.results])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward\n3            20000.0  0.001  0.900   0.0100        -264.242\n0            20000.0  0.030  0.900   0.0100           7.968\n1            20000.0  0.050  0.900   0.0100           8.074\n2            20000.0  0.100  0.900   0.0100           7.678\n4            20000.0  0.030  0.600   0.0100         -38.094\n3            20000.0  0.030  0.800   0.0100           1.814\n0            20000.0  0.030  0.950   0.0100           8.006\n1            20000.0  0.030  0.990   0.0100           7.824\n2            20000.0  0.030  0.999   0.0100           7.934\n8            20000.0  0.030  0.900   0.0000           7.748\n7            20000.0  0.030  0.900   0.0001           7.988\n6            20000.0  0.030  0.900   0.0010           8.022\n5            20000.0  0.030  0.900   0.0050           8.062\n0            20000.0  0.030  0.900   0.0500           8.006\n1            20000.0  0.030  0.900   0.1000           7.868\n2            20000.0  0.030  0.900   0.2000           7.772\n3            20000.0  0.030  0.900   0.5000           7.902\n4            20000.0  0.030  0.900   1.0000           7.906",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>-264.242</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>8.074</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.678</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.600</td>\n      <td>0.0100</td>\n      <td>-38.094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.800</td>\n      <td>0.0100</td>\n      <td>1.814</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.950</td>\n      <td>0.0100</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.990</td>\n      <td>0.0100</td>\n      <td>7.824</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.999</td>\n      <td>0.0100</td>\n      <td>7.934</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0000</td>\n      <td>7.748</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0001</td>\n      <td>7.988</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0010</td>\n      <td>8.022</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0050</td>\n      <td>8.062</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0500</td>\n      <td>8.006</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.1000</td>\n      <td>7.868</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.2000</td>\n      <td>7.772</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.5000</td>\n      <td>7.902</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>1.0000</td>\n      <td>7.906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}