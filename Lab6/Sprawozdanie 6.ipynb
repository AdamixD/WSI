{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sprawozdanie z laboratorium 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Autor: Adam Dąbkowski***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Celem szóstego laboratorium jest zaimplementowanie algorytmu ***Q-learning***. Dodatkowo należy stworzyć agenta rozwiązującego problem ***Taxi***.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Importowanie niezbędnych bibliotek"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Wizualizacja stanu środowiska"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wykorzystywane przez na środowisko zawiera cztery wyznaczone miejsca (***R***, ***G***, ***Y***, ***B***), w których pasażer może wsiąść do taksówki (***żółty prostokąt***) lub wysiąść. Gracz otrzymuje pozytywne nagrody za udane podrzucenie pasażera w odpowiednim miejscu, natomiast negatywne nagrody za próby odebrania/odwiezienia pasażera kończące się niepowodzeniem oraz za każdy krok, w którym nie otrzymano kolejnej nagrody."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001B[43m \u001B[0m|\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implementacja algorytmu ***Q-learning***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Głównym zadaniem szóstego laboratorium jest implementacja algorytmu ***Q-learning***. Po za tym należy stworzyć agenta rozwiązującego problem ***Taxi***. W tym celu stworzona została klasa ***QlearningAgent***. Podczas tworzenia obiektu tej klasy istnieje możliwość podania parametrów ***env*** (*wykorzystywane środowisko*), ***beta*** (*współczynnik uczenia*), ***gamma*** (*stopa dyskontowa*) oraz ***epsilon*** (*prawdopodobieństwo $\\epsilon$*).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Klasa ***QlearningAgent*** zawiera także cztery metody:\n",
    "- ***get_parameters()*** - metoda zwracająca wartości parametrów ***beta***, ***gamma*** oraz ***epsilon***\n",
    "- ***exploration()*** - metoda odpowiadająca za strategię eksploracji (w tym przypadku ***strategię $\\epsilon$-zachłanną***)\n",
    "- ***learn()*** - metoda odpowiadająca za uczenie według algorytmu ***Q-learning***\n",
    "- ***evaluate()*** - metoda odpowiedzialna za ocenę na danym etapie uczenia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class QlearningAgent:\n",
    "    def __init__(self, env, beta=0.03, gamma=0.9, epsilon=0.01):\n",
    "        self.env = env\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return {\n",
    "            \"beta\": self.beta,\n",
    "            \"gamma\": self.gamma,\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "\n",
    "    def exploration(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, n_episodes=20000, n_eval_episodes=500, eval_period=2000, deep_printing=False):\n",
    "        for i in range(n_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.exploration(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                self.Q[state, action] += self.beta * (reward + self.gamma * np.max(self.Q[new_state, :]) - self.Q[state, action])\n",
    "                state = new_state\n",
    "\n",
    "\n",
    "            if (i+1) % eval_period == 0 or (i+1) == n_episodes:\n",
    "                average_reward = self.evaluate(n_eval_episodes, deep_printing)\n",
    "                print(f'After {i+1}/{n_episodes} learning episodes - average reward: {average_reward}')\n",
    "                if deep_printing:\n",
    "                    print(\" \")\n",
    "\n",
    "        return average_reward\n",
    "\n",
    "\n",
    "    def evaluate(self, n_eval_episodes, printing=False):\n",
    "        all_rewards = []\n",
    "        for i in range(n_eval_episodes):\n",
    "            episode_reward = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = np.argmax(self.Q[state])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            all_rewards.append(episode_reward)\n",
    "\n",
    "            if printing:\n",
    "                print(f'Episode {i} reward: {episode_reward}')\n",
    "\n",
    "        return np.mean(all_rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aby móc w łatwy sposób prezentować i analizować rezultaty działania algorytmu dla poszczególnych przypadków, zaimplementowana została prosta klasa ***Results***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.results = pd.DataFrame(columns=[\"Learning episodes\", \"beta\", \"gamma\", \"epsilon\", \"Average reward\", \"Training time\"])\n",
    "\n",
    "    def update_results(self, n_episodes, beta, gamma, epsilon, average_reward, training_time):\n",
    "        self.results.loc[len(self.results)] = [n_episodes, beta, gamma, epsilon, average_reward, training_time]\n",
    "\n",
    "    def delete_row(self, index):\n",
    "        self.results.drop([index], axis=0, inplace=True)\n",
    "\n",
    "    def sort_results(self, column_name):\n",
    "        self.results = self.results.sort_values(by=[column_name])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.results.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Zastosowanie algorytmu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Mając zaimplementowaną klasę ***QlearningAgent*** oraz funkcje analizujące otrzymywane wyniki, możemy przejść do przeprowadzenia szeregu doświadczeń, dzięki którym zbadamy wpływ poszczególnych parametrów. Na początku ustalmy liczbę epizodów całego procesu uczenia (***n_episodes***), liczbę epizodów pojedynczej ewaluacji (***n_eval_episodes***) oraz okres, który wskazuje na częstotliwość wykonywania ewaluacji (***eval_period***). Są to wartości równe odpowiednio **20000**, **500** oraz **2000**. Poprzez zastosowanie tak dużej liczby epizodów ewaluacyjnych jesteśmy w stanie przedstawić bardziej uśrednione wyniki, a co za tym idzie, precyzyjniejsze wnioski."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "n_episodes = 20000\n",
    "n_eval_episodes = 500\n",
    "eval_period = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Badanie wpływu współczynnika $\\beta$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nasze rozważania rozpoczynamy od sprawdzenia działania algorytmu dla wartości domyślnym. W naszym przypadku są ***beta = 0,03***, ***gamma = 0,9*** oraz ***epsilon = 0,01***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "results_beta = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "{'beta': 0.03, 'gamma': 0.9, 'epsilon': 0.01}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -241.13\n",
      "After 4000/20000 learning episodes - average reward: -76.094\n",
      "After 6000/20000 learning episodes - average reward: -25.552\n",
      "After 8000/20000 learning episodes - average reward: -0.698\n",
      "After 10000/20000 learning episodes - average reward: 3.342\n",
      "After 12000/20000 learning episodes - average reward: 7.464\n",
      "After 14000/20000 learning episodes - average reward: 6.412\n",
      "After 16000/20000 learning episodes - average reward: 7.932\n",
      "After 18000/20000 learning episodes - average reward: 8.01\n",
      "After 20000/20000 learning episodes - average reward: 7.904\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jak widać, powyżej zadowalające rezultaty otrzymujemy po ok. **12000** epizodach treningowych. Sprawdźmy teraz, co się stanie, gdy wartość współczynnika uczenia $\\beta$ zostanie zwiększona do wartości **0,05**, czy algorytm będzie w stanie wcześniej osiągnąć satysfakcjonujące nas wyniki."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "beta = 0.05\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -160.512\n",
      "After 4000/20000 learning episodes - average reward: -50.546\n",
      "After 6000/20000 learning episodes - average reward: 1.396\n",
      "After 8000/20000 learning episodes - average reward: 7.712\n",
      "After 10000/20000 learning episodes - average reward: 8.118\n",
      "After 12000/20000 learning episodes - average reward: 7.826\n",
      "After 14000/20000 learning episodes - average reward: 7.772\n",
      "After 16000/20000 learning episodes - average reward: 8.02\n",
      "After 18000/20000 learning episodes - average reward: 8.05\n",
      "After 20000/20000 learning episodes - average reward: 7.888\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zgodnie z naszymi przewidywaniami, algorytm znacznie szybciej przekroczył próg o wartości **7**, ponieważ wynik ten został osiągnięty już po ok. **8000**. Chcąc otrzymać podobne rezultaty w jeszcze krótszym czasie, sprawdźmy, co się stanie, gdy współczynnik uczenia zostanie zwiększony do wartości **0,1**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "beta = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -30.722\n",
      "After 4000/20000 learning episodes - average reward: 7.11\n",
      "After 6000/20000 learning episodes - average reward: 7.892\n",
      "After 8000/20000 learning episodes - average reward: 7.922\n",
      "After 10000/20000 learning episodes - average reward: 8.234\n",
      "After 12000/20000 learning episodes - average reward: 7.932\n",
      "After 14000/20000 learning episodes - average reward: 7.876\n",
      "After 16000/20000 learning episodes - average reward: 7.87\n",
      "After 18000/20000 learning episodes - average reward: 7.94\n",
      "After 20000/20000 learning episodes - average reward: 7.818\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ponownie czas, po którym otrzymujemy świetne wyniki, został skrócony, tym razem do ok. **4000** epizodów. Oczywiście możemy domyślać się, że próg równy **7** może zostać przekroczony znacznie wcześniej, gdyż przyjęty kwant czasu (w naszym przypadku liczba epizodów uczących pomiędzy ewaluacjami) uniemożliwia dostrzeżenie wspomnianej obserwacji. Aby to udowodnić, poniżej zamieszczono wyniki doświadczenia dla mniejszej wartości ***eval_period*** równej **500**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 500/6000 learning episodes - average reward: -235.548\n",
      "After 1000/6000 learning episodes - average reward: -90.672\n",
      "After 1500/6000 learning episodes - average reward: -48.608\n",
      "After 2000/6000 learning episodes - average reward: -19.072\n",
      "After 2500/6000 learning episodes - average reward: 3.72\n",
      "After 3000/6000 learning episodes - average reward: 6.244\n",
      "After 3500/6000 learning episodes - average reward: 8.052\n",
      "After 4000/6000 learning episodes - average reward: 7.79\n",
      "After 4500/6000 learning episodes - average reward: 7.986\n",
      "After 5000/6000 learning episodes - average reward: 7.95\n",
      "After 5500/6000 learning episodes - average reward: 7.792\n",
      "After 6000/6000 learning episodes - average reward: 7.862\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=6000, n_eval_episodes=500, eval_period=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na koniec sprawdźmy, czy zmniejszenie współczynnika uczenia rzeczywiście poskutkuje znacznie wolniejszym przebiegiem uczenia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "beta = 0.001\n",
    "gamma = 0.9\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -232.364\n",
      "After 4000/20000 learning episodes - average reward: -289.424\n",
      "After 6000/20000 learning episodes - average reward: -364.754\n",
      "After 8000/20000 learning episodes - average reward: -268.364\n",
      "After 10000/20000 learning episodes - average reward: -261.002\n",
      "After 12000/20000 learning episodes - average reward: -342.524\n",
      "After 14000/20000 learning episodes - average reward: -332.534\n",
      "After 16000/20000 learning episodes - average reward: -353.504\n",
      "After 18000/20000 learning episodes - average reward: -331.8\n",
      "After 20000/20000 learning episodes - average reward: -335.238\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "results_beta.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jak widać, nasze przypuszczenia ponownie się potwierdziły. Dla ***beta*** równej **0,001** algorytm nie jest w stanie nawet osiągnąć wartości dodatniej."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poniżej zamieszona jest zbiorcza tabela zawierająca wyniki dotychczasowych doświadczeń."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "results_beta.sort_results(\"beta\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward  Training time\n3            20000.0  0.001    0.9     0.01        -335.238      96.062129\n0            20000.0  0.030    0.9     0.01           7.904      16.596551\n1            20000.0  0.050    0.9     0.01           7.888      12.511721\n2            20000.0  0.100    0.9     0.01           7.818      11.106120",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n      <th>Training time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>-335.238</td>\n      <td>96.062129</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.904</td>\n      <td>16.596551</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.888</td>\n      <td>12.511721</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.9</td>\n      <td>0.01</td>\n      <td>7.818</td>\n      <td>11.106120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_beta.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jak widać powyżej, tabela oprócz wartości parametrów opisanych powyżej zawiera dodatkowo kolumnę, w której zamieszczono czas uczenia (włącznie z ewaluacją) dla poszczególnych zestawów parametrów. Łatwo zauważyć, że wraz ze wzrostem współczynnika $\\beta$ odnotowujemy krótszy czas potrzebny na osiągnięcie końcowego wyniku. Zdecydowanie najgorzej pod tym względem wypada zestaw z najmniejszą wartością współczynnika ***beta***."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Badanie wpływu współczynnika $\\gamma$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kolejnym z badanych parametrów jest stopa dyskontowa **$\\gamma$**. Jeśli jej wartość będzie bliska **0**, wówczas przyszłe nagrody będą miały marginalne znaczenie w stosunku do bieżących nagród, natomiast jeśli **$\\gamma$** będzie przyjmować wartości zbliżające się do **1**, to przyszłe nagrody będą miały niemal identyczne znaczenie co nagrody bieżące."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na początku sprawdźmy, czy zwiększenie stopy dyskontowej będzie pozytywnie wpływało na szybkość uczenia. W tym celu przeprowadzimy trzy doświadczenia, w których $\\gamma$ równa będzie **0,95**, **0,99** oraz **0,999**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "results_gamma = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.95\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -249.972\n",
      "After 4000/20000 learning episodes - average reward: -86.492\n",
      "After 6000/20000 learning episodes - average reward: -23.058\n",
      "After 8000/20000 learning episodes - average reward: 0.482\n",
      "After 10000/20000 learning episodes - average reward: 7.73\n",
      "After 12000/20000 learning episodes - average reward: 7.864\n",
      "After 14000/20000 learning episodes - average reward: 7.966\n",
      "After 16000/20000 learning episodes - average reward: 8.202\n",
      "After 18000/20000 learning episodes - average reward: 7.76\n",
      "After 20000/20000 learning episodes - average reward: 7.942\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.99\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -239.664\n",
      "After 4000/20000 learning episodes - average reward: -87.298\n",
      "After 6000/20000 learning episodes - average reward: -51.04\n",
      "After 8000/20000 learning episodes - average reward: -18.342\n",
      "After 10000/20000 learning episodes - average reward: 7.394\n",
      "After 12000/20000 learning episodes - average reward: 7.742\n",
      "After 14000/20000 learning episodes - average reward: 7.984\n",
      "After 16000/20000 learning episodes - average reward: 7.908\n",
      "After 18000/20000 learning episodes - average reward: 7.794\n",
      "After 20000/20000 learning episodes - average reward: 7.938\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.999\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -322.798\n",
      "After 4000/20000 learning episodes - average reward: -98.87\n",
      "After 6000/20000 learning episodes - average reward: -15.416\n",
      "After 8000/20000 learning episodes - average reward: 5.058\n",
      "After 10000/20000 learning episodes - average reward: 8.076\n",
      "After 12000/20000 learning episodes - average reward: 7.728\n",
      "After 14000/20000 learning episodes - average reward: 7.714\n",
      "After 16000/20000 learning episodes - average reward: 7.984\n",
      "After 18000/20000 learning episodes - average reward: 8.134\n",
      "After 20000/20000 learning episodes - average reward: 7.886\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O ile nie dostrzegamy poważnych różnic pomiędzy wynikami dla trzech powyższych symulacji, o tyle łatwo zauważyć poprawę względem wartości, które przyjęliśmy za domyślne. Sprawdźmy zatem, czy zmniejszenie współczynnika ***gamma*** względem wartości początkowej, będzie negatywne w skutkach."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.8\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -309.412\n",
      "After 4000/20000 learning episodes - average reward: -135.292\n",
      "After 6000/20000 learning episodes - average reward: -86.72\n",
      "After 8000/20000 learning episodes - average reward: -30.642\n",
      "After 10000/20000 learning episodes - average reward: -19.504\n",
      "After 12000/20000 learning episodes - average reward: -11.708\n",
      "After 14000/20000 learning episodes - average reward: -7.332\n",
      "After 16000/20000 learning episodes - average reward: -3.884\n",
      "After 18000/20000 learning episodes - average reward: 1.5\n",
      "After 20000/20000 learning episodes - average reward: -3.672\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.6\n",
    "epsilon = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -218.396\n",
      "After 4000/20000 learning episodes - average reward: -149.81\n",
      "After 6000/20000 learning episodes - average reward: -110.17\n",
      "After 8000/20000 learning episodes - average reward: -106.014\n",
      "After 10000/20000 learning episodes - average reward: -95.364\n",
      "After 12000/20000 learning episodes - average reward: -81.576\n",
      "After 14000/20000 learning episodes - average reward: -57.154\n",
      "After 16000/20000 learning episodes - average reward: -45.768\n",
      "After 18000/20000 learning episodes - average reward: -46.934\n",
      "After 20000/20000 learning episodes - average reward: -40.864\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "results_gamma.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zarówno w jednym, jak i drugim przypadku wyniki są zauważalnie gorsze, gdyż nie jesteśmy w stanie uzyskać dodatniego finalnego wyniku dla danej liczby epizodów procesu uczenia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "results_gamma.sort_results(\"gamma\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward  Training time\n4            20000.0  0.03  0.600     0.01         -40.864      22.380610\n3            20000.0  0.03  0.800     0.01          -3.672      16.822754\n0            20000.0  0.03  0.950     0.01           7.942      14.996722\n1            20000.0  0.03  0.990     0.01           7.938      15.066344\n2            20000.0  0.03  0.999     0.01           7.886      13.992574",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n      <th>Training time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.600</td>\n      <td>0.01</td>\n      <td>-40.864</td>\n      <td>22.380610</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.800</td>\n      <td>0.01</td>\n      <td>-3.672</td>\n      <td>16.822754</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.950</td>\n      <td>0.01</td>\n      <td>7.942</td>\n      <td>14.996722</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.990</td>\n      <td>0.01</td>\n      <td>7.938</td>\n      <td>15.066344</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.999</td>\n      <td>0.01</td>\n      <td>7.886</td>\n      <td>13.992574</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gamma.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W tym przypadku również jesteśmy w stanie dostrzec pewną prawidłowość dotyczącą czasu trwania uczenia. Tym razem to wzrost stopy dyskontowej powoduje skrócenie potrzebnego czasu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Badanie wpływu wartości parametru $\\epsilon$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mając na uwadze fakt, że przyjętą przez nas strategią eksploracji jest strategia $\\epsilon$ - zachłanna, istotnym parametrem jest $\\epsilon$ - prawdopodobieństwo wyboru losowego, przy prawdopodobieństwie wyboru czynności o największej Q-wartości równym 1-$\\epsilon$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chcąc sprawdzić, czy zwiększenie eksploracji poprzez zwiększenie losowości pozytywnie wpłynie na czas otrzymywania satysfakcjonujących wyników, przeprowadzone zostaną doświadczenia o parametrze ***epsilon*** równym kolejno **0,05**, **0,1**, **0,2**, **0,5** oraz **1**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "results_epsilon = Results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -269.134\n",
      "After 4000/20000 learning episodes - average reward: -146.474\n",
      "After 6000/20000 learning episodes - average reward: -23.794\n",
      "After 8000/20000 learning episodes - average reward: -0.398\n",
      "After 10000/20000 learning episodes - average reward: -1.988\n",
      "After 12000/20000 learning episodes - average reward: 6.326\n",
      "After 14000/20000 learning episodes - average reward: 7.756\n",
      "After 16000/20000 learning episodes - average reward: 8.012\n",
      "After 18000/20000 learning episodes - average reward: 7.872\n",
      "After 20000/20000 learning episodes - average reward: 7.768\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -167.234\n",
      "After 4000/20000 learning episodes - average reward: -94.964\n",
      "After 6000/20000 learning episodes - average reward: -24.282\n",
      "After 8000/20000 learning episodes - average reward: -7.204\n",
      "After 10000/20000 learning episodes - average reward: 2.556\n",
      "After 12000/20000 learning episodes - average reward: 5.042\n",
      "After 14000/20000 learning episodes - average reward: 8.062\n",
      "After 16000/20000 learning episodes - average reward: 7.89\n",
      "After 18000/20000 learning episodes - average reward: 7.986\n",
      "After 20000/20000 learning episodes - average reward: 7.948\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -157.27\n",
      "After 4000/20000 learning episodes - average reward: -83.16\n",
      "After 6000/20000 learning episodes - average reward: -24.73\n",
      "After 8000/20000 learning episodes - average reward: -7.616\n",
      "After 10000/20000 learning episodes - average reward: 3.09\n",
      "After 12000/20000 learning episodes - average reward: 6.996\n",
      "After 14000/20000 learning episodes - average reward: 7.698\n",
      "After 16000/20000 learning episodes - average reward: 8.15\n",
      "After 18000/20000 learning episodes - average reward: 7.892\n",
      "After 20000/20000 learning episodes - average reward: 7.912\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -166.172\n",
      "After 4000/20000 learning episodes - average reward: -73.556\n",
      "After 6000/20000 learning episodes - average reward: -13.212\n",
      "After 8000/20000 learning episodes - average reward: -2.132\n",
      "After 10000/20000 learning episodes - average reward: 4.328\n",
      "After 12000/20000 learning episodes - average reward: 7.928\n",
      "After 14000/20000 learning episodes - average reward: 7.872\n",
      "After 16000/20000 learning episodes - average reward: 8.006\n",
      "After 18000/20000 learning episodes - average reward: 7.952\n",
      "After 20000/20000 learning episodes - average reward: 7.938\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -166.674\n",
      "After 4000/20000 learning episodes - average reward: 7.266\n",
      "After 6000/20000 learning episodes - average reward: 7.664\n",
      "After 8000/20000 learning episodes - average reward: 8.094\n",
      "After 10000/20000 learning episodes - average reward: 8.01\n",
      "After 12000/20000 learning episodes - average reward: 8.1\n",
      "After 14000/20000 learning episodes - average reward: 7.972\n",
      "After 16000/20000 learning episodes - average reward: 7.702\n",
      "After 18000/20000 learning episodes - average reward: 8.09\n",
      "After 20000/20000 learning episodes - average reward: 7.914\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otrzymane wyniki w większości nie odbiegają znacząco od tych, które otrzymaliśmy dla wartości początkowych, ponieważ zadowalające wartości otrzymujemy w przybliżeniu po ok. **12000 - 14000** epizodach uczenia. Ciekawe rezultaty otrzymaliśmy natomiast dla ***epsilona*** równego 1, wskazującego na całkowicie losowe podejście do strategi eksploracji. O dziwo wyniki są rewelacyjne. Jednakże wynika to głównie ze szczęśliwego trafu i specyfiki problemu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wiedząc, jaki wpływ ma duża wartość parametru ***epsilon***, sprawdźmy, co się stanie, gdy losowość podczas eksploracji zostanie zmniejszona. W tym celu przeprowadzone zostaną doświadczenia dla $\\epsilon$ równego **0,005**, **0,001**, **0,0001** oraz **0**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.005"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -172.674\n",
      "After 4000/20000 learning episodes - average reward: -88.818\n",
      "After 6000/20000 learning episodes - average reward: -26.568\n",
      "After 8000/20000 learning episodes - average reward: -2.516\n",
      "After 10000/20000 learning episodes - average reward: 5.088\n",
      "After 12000/20000 learning episodes - average reward: 6.35\n",
      "After 14000/20000 learning episodes - average reward: 7.882\n",
      "After 16000/20000 learning episodes - average reward: 8.08\n",
      "After 18000/20000 learning episodes - average reward: 7.844\n",
      "After 20000/20000 learning episodes - average reward: 7.914\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -227.332\n",
      "After 4000/20000 learning episodes - average reward: -97.596\n",
      "After 6000/20000 learning episodes - average reward: -25.944\n",
      "After 8000/20000 learning episodes - average reward: -13.264\n",
      "After 10000/20000 learning episodes - average reward: -13.658\n",
      "After 12000/20000 learning episodes - average reward: 5.814\n",
      "After 14000/20000 learning episodes - average reward: 7.948\n",
      "After 16000/20000 learning episodes - average reward: 7.906\n",
      "After 18000/20000 learning episodes - average reward: 7.908\n",
      "After 20000/20000 learning episodes - average reward: 7.874\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -212.84\n",
      "After 4000/20000 learning episodes - average reward: -141.592\n",
      "After 6000/20000 learning episodes - average reward: -30.74\n",
      "After 8000/20000 learning episodes - average reward: -4.714\n",
      "After 10000/20000 learning episodes - average reward: 2.566\n",
      "After 12000/20000 learning episodes - average reward: 7.824\n",
      "After 14000/20000 learning episodes - average reward: 7.766\n",
      "After 16000/20000 learning episodes - average reward: 7.836\n",
      "After 18000/20000 learning episodes - average reward: 8.096\n",
      "After 20000/20000 learning episodes - average reward: 7.992\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "beta = 0.03\n",
    "gamma = 0.9\n",
    "epsilon = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "agent = QlearningAgent(env=env, beta=beta, gamma=gamma, epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "timer_start = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 2000/20000 learning episodes - average reward: -188.192\n",
      "After 4000/20000 learning episodes - average reward: -88.032\n",
      "After 6000/20000 learning episodes - average reward: -54.956\n",
      "After 8000/20000 learning episodes - average reward: -12.762\n",
      "After 10000/20000 learning episodes - average reward: 5.016\n",
      "After 12000/20000 learning episodes - average reward: 7.524\n",
      "After 14000/20000 learning episodes - average reward: 7.394\n",
      "After 16000/20000 learning episodes - average reward: 7.954\n",
      "After 18000/20000 learning episodes - average reward: 7.93\n",
      "After 20000/20000 learning episodes - average reward: 7.772\n"
     ]
    }
   ],
   "source": [
    "reward = agent.learn(n_episodes=n_episodes, n_eval_episodes=n_eval_episodes, eval_period=eval_period)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "results_epsilon.update_results(n_episodes, beta, gamma, epsilon, reward, time.time() - timer_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otrzymane rezultaty przypominają te, które obserwowaliśmy dla większych wartości ***epsilon***. Dzięki temu wiemy, żeby wpływ parametru $\\epsilon$ nie jest tak duży, jak ma to miejsce dla współczynnika uczenia czy stopy dyskontowej."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Poniżej, analogicznie jak wcześniej, zamieszczona została tabela zawierająca wyniki przeprowadzone w omawianym podpunkcie."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "results_epsilon.sort_results(\"epsilon\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes  beta  gamma  epsilon  Average reward  Training time\n8            20000.0  0.03    0.9   0.0000           7.772      14.990079\n7            20000.0  0.03    0.9   0.0001           7.992      15.041967\n6            20000.0  0.03    0.9   0.0010           7.874      14.859613\n5            20000.0  0.03    0.9   0.0050           7.914      15.160997\n0            20000.0  0.03    0.9   0.0500           7.768      15.504930\n1            20000.0  0.03    0.9   0.1000           7.948      16.062076\n2            20000.0  0.03    0.9   0.2000           7.912      17.421455\n3            20000.0  0.03    0.9   0.5000           7.938      24.528253\n4            20000.0  0.03    0.9   1.0000           7.914     109.131963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n      <th>Training time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0000</td>\n      <td>7.772</td>\n      <td>14.990079</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0001</td>\n      <td>7.992</td>\n      <td>15.041967</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0010</td>\n      <td>7.874</td>\n      <td>14.859613</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0050</td>\n      <td>7.914</td>\n      <td>15.160997</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.0500</td>\n      <td>7.768</td>\n      <td>15.504930</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.1000</td>\n      <td>7.948</td>\n      <td>16.062076</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.2000</td>\n      <td>7.912</td>\n      <td>17.421455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>0.5000</td>\n      <td>7.938</td>\n      <td>24.528253</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.03</td>\n      <td>0.9</td>\n      <td>1.0000</td>\n      <td>7.914</td>\n      <td>109.131963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_epsilon.results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analizując powyższą tabelę, dostrzegamy jednak, że przyjmowanie dużych wartości $\\epsilon$ nie pozostaje bez echa. Z im większą jego wartością mamy do czynienia, tym wyraźniej wzrasta czas potrzebny na uzyskanie końcowego wyniku, co szczególnie widać dla ***epsilon*** równego **0,5** i **1**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Podsumowanie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Dla zwiększenia czytelności otrzymanych rezultatów, poniżej została zamieszczona zbiorcza tabela, która przedstawia wyniki powyższych doświadczeń."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "results = pd.concat([results_beta.results, results_gamma.results, results_epsilon.results])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "   Learning episodes   beta  gamma  epsilon  Average reward  Training time\n3            20000.0  0.001  0.900   0.0100        -335.238      96.062129\n0            20000.0  0.030  0.900   0.0100           7.904      16.596551\n1            20000.0  0.050  0.900   0.0100           7.888      12.511721\n2            20000.0  0.100  0.900   0.0100           7.818      11.106120\n4            20000.0  0.030  0.600   0.0100         -40.864      22.380610\n3            20000.0  0.030  0.800   0.0100          -3.672      16.822754\n0            20000.0  0.030  0.950   0.0100           7.942      14.996722\n1            20000.0  0.030  0.990   0.0100           7.938      15.066344\n2            20000.0  0.030  0.999   0.0100           7.886      13.992574\n8            20000.0  0.030  0.900   0.0000           7.772      14.990079\n7            20000.0  0.030  0.900   0.0001           7.992      15.041967\n6            20000.0  0.030  0.900   0.0010           7.874      14.859613\n5            20000.0  0.030  0.900   0.0050           7.914      15.160997\n0            20000.0  0.030  0.900   0.0500           7.768      15.504930\n1            20000.0  0.030  0.900   0.1000           7.948      16.062076\n2            20000.0  0.030  0.900   0.2000           7.912      17.421455\n3            20000.0  0.030  0.900   0.5000           7.938      24.528253\n4            20000.0  0.030  0.900   1.0000           7.914     109.131963",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning episodes</th>\n      <th>beta</th>\n      <th>gamma</th>\n      <th>epsilon</th>\n      <th>Average reward</th>\n      <th>Training time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.001</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>-335.238</td>\n      <td>96.062129</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.904</td>\n      <td>16.596551</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.050</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.888</td>\n      <td>12.511721</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.100</td>\n      <td>0.900</td>\n      <td>0.0100</td>\n      <td>7.818</td>\n      <td>11.106120</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.600</td>\n      <td>0.0100</td>\n      <td>-40.864</td>\n      <td>22.380610</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.800</td>\n      <td>0.0100</td>\n      <td>-3.672</td>\n      <td>16.822754</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.950</td>\n      <td>0.0100</td>\n      <td>7.942</td>\n      <td>14.996722</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.990</td>\n      <td>0.0100</td>\n      <td>7.938</td>\n      <td>15.066344</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.999</td>\n      <td>0.0100</td>\n      <td>7.886</td>\n      <td>13.992574</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0000</td>\n      <td>7.772</td>\n      <td>14.990079</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0001</td>\n      <td>7.992</td>\n      <td>15.041967</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0010</td>\n      <td>7.874</td>\n      <td>14.859613</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0050</td>\n      <td>7.914</td>\n      <td>15.160997</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.0500</td>\n      <td>7.768</td>\n      <td>15.504930</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.1000</td>\n      <td>7.948</td>\n      <td>16.062076</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.2000</td>\n      <td>7.912</td>\n      <td>17.421455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>0.5000</td>\n      <td>7.938</td>\n      <td>24.528253</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20000.0</td>\n      <td>0.030</td>\n      <td>0.900</td>\n      <td>1.0000</td>\n      <td>7.914</td>\n      <td>109.131963</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mając na uwadze dotychczasowe obserwacje, a także wyniki zamieszczone w powyższej tabeli, możemy dojść do szeregu wniosków. Pierwszym z nich jest fakt, że wraz ze wzrostem współczynnika uczenia $\\beta$ jesteśmy w stanie znacząco zredukować liczbę epizodów koniecznych do osiągnięcia satysfakcjonujących nas rezultatów. W przypadku jego zbyt małej wartości może zdarzyć się, że już samo osiągnięcie dodatniego wyniku będzie wynikiem niemożliwym. Podobną sytuację możemy zaobserwować przypadku stopy dyskontowej $\\gamma$. Tutaj również w przypadku zbyt małej wartości nie jesteśmy w stanie osiągnąć dodatniego wyniku, nie mówiąc już o przekroczeniu progu, z którego możemy być zadowoleni."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inaczej wygląda sytuacja z parametrem $\\epsilon$. W tym przypadku, na podstawie otrzymanych rezultatów, nie zauważamy istotnego wpływu wartości wspomnianego parametru na jakość uczenia, w tym wymaganą liczbę epizodów treningowych. Warto jednak wspomnieć, że wraz ze wzrostem wartości parametru ***epsilon*** obserwujemy dłuższy czas potrzebny na ukończenie pełnego procesu uczenia. Oczywiście w wyniku doświadczenia, w którym $\\epsilon$ był równy **1**, otrzymujemy rewelacyjne rezultaty już po ok. **4000** epizodów, jednakże otrzymany wynik należy skonfrontować z czasem, który był potrzebny na wykonanie **20000** epizodów. Czas ten jest zdecydowanie najdłuższy. W przypadku pozostałych parametrów również obserwujemy pewne zależności związane z czasem wykonania, jednakże w przeciwieństwie do parametru $\\epsilon$, na zwiększenie czasu uczenia wpływają coraz mniejsze wartości parametrów $\\beta$ i $\\gamma$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na potrzeby zestawienia ze sobą rezultatów dla różnych zestawów parametrów, w celu zbadania wpływu poszczególnego parametru i uzyskania miarodajnego wyniku, przyjęto tę samą liczbę epizodów uczących i ewaluacyjnych w trakcie jednego cyklu. Jednakże przy finalnym doborze parametrów, należy wziąć pod uwagę to, że w większości przypadków satysfakcjonujący nas wynik jesteśmy w stanie uzyskać po znacznie mniejszej ilości epizodów treningowych, niż **20000**. Wówczas powinniśmy wziąć poprawkę, że rzeczywisty czas potrzebny na osiągnięcie takiego wyniku jest znacznie krótszy. W związku z tym możemy stwierdzić, że najlepszym z rozważanych przez nas zestawów parametrów jest ten gdzie $\\beta$ jest równa **0,1**, $\\gamma$ wynosi **0,9**, natomiast $\\epsilon$ przyjmuje wartość **0,01**. Uzyskujemy wówczas czas uczenia wynoszący **11,106 s**, jednakże mając na uwadze fakt, że dobry wynik uzyskaliśmy już po ok. **4000** epizodów treningowych, możemy założyć, że zużycie zasobów, jakim niewątpliwie jest czas, również zostanie zredukowane.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}